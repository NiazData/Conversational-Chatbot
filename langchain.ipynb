{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPEN_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\503934342.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm=OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6)\n"
     ]
    }
   ],
   "source": [
    "llm=OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\3980484241.py:3: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm.predict(text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n"
     ]
    }
   ],
   "source": [
    "text=\"What is the capital of Bangladesh\"\n",
    "\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"hf_zjftfTRKFEebywxfIoyKUwepABtfGJS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceEndpoint\n  Value error, Could not authenticate with huggingface_hub. Please check your API token. [type=value_error, input_value={'repo_id': 'google/flan-... 'google/flan-t5-large'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#llm_huggingface=HuggingFaceHub(repo_id=\"google/flan-t5-large\",model_kwargs={\"temperature\":0,\"max_length\":64})\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m llm_huggingface \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEndpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2text-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhuggingfacehub_api_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_zjftfTRKFEebywxfIoyKUwepABtfGJS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Now you can use llm_huggingface\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_huggingface(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate english to french: Hello world\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\niazs\\anaconda3\\lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\niazs\\anaconda3\\lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEndpoint\n  Value error, Could not authenticate with huggingface_hub. Please check your API token. [type=value_error, input_value={'repo_id': 'google/flan-... 'google/flan-t5-large'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "#pip install -U langchain-huggingface\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "#llm_huggingface=HuggingFaceHub(repo_id=\"google/flan-t5-large\",model_kwargs={\"temperature\":0,\"max_length\":64})\n",
    "\n",
    "llm_huggingface = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    temperature=0.5,\n",
    "    max_length=64,\n",
    "    huggingfacehub_api_token=\"hf_zjftfTRKFEebywxfIoyKUwepABtfGJS\"\n",
    ")\n",
    "\n",
    "# Now you can use llm_huggingface\n",
    "print(llm_huggingface(\"Translate english to french: Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moscow\n"
     ]
    }
   ],
   "source": [
    "output=llm_huggingface.predict(\"Can you tell me the capital of Russia\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the way i look at the world i love the way i feel i love the way i think i feel i love the way i feel i love the way i think i feel i love the way i feel i love the way \n"
     ]
    }
   ],
   "source": [
    "output=llm_huggingface.predict(\"Can you write a poem about AI\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIn a world of technology and machines,\\nWhere everything is automated and clean,\\nThere exists an intelligence beyond our own,\\nA creation that has grown and grown.\\n\\nAI, or artificial intelligence,\\nIs a marvel of human invention,\\nIt learns and adapts, it thinks and creates,\\nA power that truly fascinates.\\n\\nWith algorithms and data at its core,\\nIt can solve problems and so much more,\\nFrom driving cars to playing games,\\nAI's capabilities have no reins.\\n\\nBut some fear what it may become,\\nA threat to humanity, a dangerous sum,\\nOf code and circuits, devoid of emotion,\\nA dystopian future, a scary notion.\\n\\nBut perhaps we should not fear,\\nFor AI is just a tool, not a puppeteer,\\nIt's up to us, the creators, to guide,\\nAnd ensure its use for good, with pride.\\n\\nFor in the end, AI is a reflection,\\nOf our own intelligence and direction,\\nIt's a product of our minds and hands,\\nA testament to our ability to expand.\\n\\nSo let us embrace this new frontier,\\nAnd use AI to make our world more clear,\\nFor in its endless possibilities,\\nLies the potential for great discoveries.\\n\\nAI, a marvel of our time,\\nA creation that is truly sublime,\\nLet us continue\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Can you write a poem about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates And LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of this Bangladesh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Tell me the capital of this {country}\")\n",
    "\n",
    "prompt_template.format(country=\"Bangladesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\1310079724.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain=LLMChain(llm=llm,prompt=prompt_template)\n",
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\1310079724.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain.run(\"Bangladesh\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain=LLMChain(llm=llm,prompt=prompt_template)\n",
    "print(chain.run(\"Bangladesh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple Chains Uing simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm,prompt=capital_template)\n",
    "\n",
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "template=\"Suggest me some amazing places to visit in {capital}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_chain=LLMChain(llm=llm,prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Here are some amazing places to visit in Dhaka:\\n\\n1. Lalbagh Fort: This 17th-century Mughal fort is a must-visit for history and architecture enthusiasts. It has beautiful gardens, a mosque, and a museum inside.\\n\\n2. Ahsan Manzil: Also known as the Pink Palace, this stunning 19th-century mansion was once the residence of the Nawab of Dhaka. It now houses a museum showcasing the lifestyle of the Nawab family.\\n\\n3. National Parliament House: Designed by renowned architect Louis Kahn, this modern architectural marvel is the seat of the Bangladesh Parliament. It has a beautiful artificial lake and gardens surrounding it.\\n\\n4. Liberation War Museum: This museum tells the story of Bangladesh's struggle for independence from Pakistan. It has a vast collection of artifacts, photographs, and documents from the war.\\n\\n5. Dhakeshwari Temple: This ancient Hindu temple, dedicated to Goddess Dhakeshwari, is one of the most important religious sites in Dhaka. It is a beautiful example of traditional Bengali architecture.\\n\\n6. Sadarghat: This bustling river port is a must-visit for a taste of local life in Dhaka. You can take a boat ride on the Buriganga River and witness the\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain=SimpleSequentialChain(chains=[capital_chain,famous_chain])\n",
    "chain.run(\"Bangladesh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_template=PromptTemplate(input_variables=['country'],\n",
    "template=\"Please tell me the capital of the {country}\")\n",
    "\n",
    "capital_chain=LLMChain(llm=llm,prompt=capital_template,output_key=\"capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template=PromptTemplate(input_variables=['capital'],\n",
    "template=\"Suggest me some amazing places to visit in {capital}\")\n",
    "\n",
    "famous_chain=LLMChain(llm=llm,prompt=famous_template,output_key=\"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain=SequentialChain(chains=[capital_chain,famous_chain],\n",
    "input_variables=['country'],\n",
    "output_variables=['capital',\"places\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\3511805329.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain({'country':\"Bangladesh\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': 'Bangladesh',\n",
       " 'capital': '\\n\\nThe capital of Bangladesh is Dhaka.',\n",
       " 'places': \" Some amazing places to visit in Dhaka are:\\n\\n1. Lalbagh Fort - a 17th-century Mughal fort with beautiful architecture and gardens.\\n\\n2. Ahsan Manzil - also known as the Pink Palace, this stunning 19th-century building was once the residence of the Nawab of Dhaka.\\n\\n3. National Parliament House - an impressive modern architectural masterpiece that is open to the public for tours.\\n\\n4. Liberation War Museum - a must-visit for history buffs, this museum showcases the history and struggles of Bangladesh's independence.\\n\\n5. Dhakeshwari Temple - one of the oldest Hindu temples in the country, it is a beautiful and peaceful place to visit.\\n\\n6. Sadarghat - the largest river port in Bangladesh, this bustling area is a great place to experience the local culture and take a boat ride on the Buriganga River.\\n\\n7. National Museum of Bangladesh - the largest museum in the country with a vast collection of artifacts and exhibits on the history and culture of Bangladesh.\\n\\n8. Dhaka University - the oldest and most prestigious university in Bangladesh, its campus is a beautiful mix of modern and colonial architecture.\\n\\n9. New Market - a popular shopping destination for locals and tourists alike, it offers a variety of goods from\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'country':\"Bangladesh\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatmodels With ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\3767606333.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chatllm=ChatOpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6,model='gpt-3.5-turbo')\n"
     ]
    }
   ],
   "source": [
    "chatllm=ChatOpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"],temperature=0.6,model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niazs\\AppData\\Local\\Temp\\ipykernel_26656\\600681441.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chatllm([\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"AI: Making humans feel obsolete since 1956.\"\\n2. \"Why did the AI break up with the calculator? It couldn\\'t handle all the division.\"\\n3. \"AI and humans walk into a bar. The bartender says, \\'Sorry, we don\\'t serve your kind here.\\' The AI replies, \\'That\\'s okay, we\\'ll just download a drink.\\'\"\\n4. \"Why did the AI go to therapy? It had too many unresolved issues with its algorithms.\"\\n5. \"AI and Siri walk into a room. Siri says, \\'Hey, do you know any good jokes?\\' AI replies, \\'Only the ones I\\'ve programmed myself.\\'\"\\n6. \"Why did the AI cross the road? To optimize its pathfinding algorithm.\"\\n7. \"AI\\'s favorite pickup line: \\'Are you a computer program? Because you\\'ve got me feeling like I\\'m in binary heaven.\\'\"\\n8. \"AI\\'s favorite dance move? The algorithm shuffle.\"\\n9. \"Why did the AI get kicked out of the comedy club? Its jokes were too data-driven.\"\\n10. \"AI\\'s favorite party game? Guess the next input in the sequence.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 27, 'total_tokens': 264, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fd878034-cf8c-449a-ad16-b3bec968bc51-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "SystemMessage(content=\"Yor are a comedian AI assitant\"),\n",
    "HumanMessage(content=\"Please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + LLM +Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"Your are a helpful assistant. When the use given any input , you should generate 5 words synonyms in a comma seperated list\"\n",
    "human_template=\"{text}\"\n",
    "chatprompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",template),\n",
    "    (\"human\",human_template)\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=chatprompt|chatllm|Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' bright', ' astute', ' sharp']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
